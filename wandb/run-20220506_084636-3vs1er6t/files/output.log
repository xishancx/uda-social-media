/var/folders/r6/sx7lg3wj1lj0bd7v7gjjl7rm0000gn/T/ipykernel_52050/3310326985.py:32: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.
  torch.nn.utils.clip_grad_norm(model.parameters(),1.0)
<xml><var name="AdamW" type="type" qualifier="builtins" value="%3Cclass %27transformers.optimization.AdamW%27&gt;" isContainer="True" />
<var name="BertForSequenceClassification" type="type" qualifier="builtins" value="%3Cclass %27transformers.models.bert.modeling_bert.BertForSequenceClassification%27&gt;" isContainer="True" />
<var name="BertTokenizer" type="type" qualifier="builtins" value="%3Cclass %27transformers.models.bert.tokenization_bert.BertTokenizer%27&gt;" isContainer="True" />
<var name="DataLoader" type="type" qualifier="builtins" value="%3Cclass %27torch.utils.data.dataloader.DataLoader%27&gt;" isContainer="True" />
<var name="In" type="list" qualifier="builtins" value="%5B%27%27%2C %27import numpy as np%5Cnimport pandas as pd%5Cnfrom utils import %2A%5Cnimport torch%5Cnfrom transformers import BertTokenizer%5Cnfrom torch.utils.data import TensorDataset%27%2C %27import numpy as np%5Cnimport pandas as pd%5Cnfrom utils import %2A%5Cnimport torch%5Cnfrom transformers import BertTokenizer%5Cnfrom torch.utils.data import TensorDataset%27%2C %27data_df = get_input_df%28%29%5Cndata_df = data_df.sample%28frac=0.001%29%27%2C %27data_df.head%28%29%27%2C %27data_df.size%27%2C %27from sklearn.model_selection import train_test_split%5CnX_train%2CX_test%2Cy_train%2Cy_test = train_test_split%28data_df.index.values%2C%5Cn                                                data_df.Target.values%2C%5Cn                                                test_size = 0.1%2C%5Cn                                                random_state=5%2C%5Cn                                                stratify = data_df.Target.values%29%27%2C %22data_df.loc%5BX_train%2C%27data_type%27%5D = %27train%27%5Cndata_df.loc%5BX_test%2C%27data_type%27%5D = %27test%27%22%2C %22data_df.groupby%28%5B%27data_type%27%2C %27Target%27%5D%29.count%28%29%22%2C %22tokenizer = Bert..." isContainer="True" shape="25" isIPythonHidden="True" />
<var name="Out" type="dict" qualifier="builtins" value="%7B4%3A                  ID                                              Input  Target%0A1262547  1998859621  %40RSprung YES. let%27s do it. we%27ll chat soon abo...       1%0A813847   1548911428  Back home%21  Bought a yummy sandwich on my way ...       1%0A544315   2201072419  uh oh. my face feels a bit feverish. took my t...       0%0A975173   1833483041  %40katfishh they promised the picture a live cha...       1%0A1020704  1882429722  %40soully loved it  makes me want to watch the f...       1%2C 5%3A 4800%2C 8%3A                    ID  Input%0Adata_type Target            %0Atest      0        80     80%0A          1        80     80%0Atrain     0       720    720%0A          1       720    720%2C 12%3A 2%2C 13%3A %281440%2C 160%29%7D" isContainer="True" shape="5" isIPythonHidden="True" />
<var name="RandomSampler" type="type" qualifier="builtins" value="%3Cclass %27torch.utils.data.sampler.RandomSampler%27&gt;" isContainer="True" />
<var name="SequentialSampler" type="type" qualifier="builtins" value="%3Cclass %27torch.utils.data.sampler.SequentialSampler%27&gt;" isContainer="True" />
<var name="TensorDataset" type="type" qualifier="builtins" value="%3Cclass %27torch.utils.data.dataset.TensorDataset%27&gt;" isContainer="True" />
<var name="X_test" type="ndarray" qualifier="numpy" value="%5B 291963  747307  680885  936315 1415731  104604 1507196  540998 1288956%2C  160105  116632 1108379 1252841 1105488  407407  405200 1215428  548478%2C  890188 1332064  879348 1068675  499669  307742 1251213  901543 1321411%2C 1547913  102081  791274  196068 1334742  629443 1374839 1212707 1581822%2C 1589971  707315 1057231 1159938  966656  190757 1521408 1374126  299431%2C  257161  562976  254292  198652 1564235 1325461  113344  451802  254946%2C  966234  786486  987729 1387043  759938 1271668  159731 1164453  906603%2C  670576  510218  174731  753673 1210853 1482968 1490695  938656  119023%2C 1441200 1033123 1593780  328331  702977  152728    6777   56453  495881%2C 1597348  304727 1554234  504661 1181885  906294  605458 1387233 1094226%2C  212512 1037152  738128  868192  143117  931238  825346 1160897  113855%2C  246983%5D" isContainer="True" shape="(160,)" />
<var name="X_train" type="ndarray" qualifier="numpy" value="%5B  14888  179298  811528  540806  686754 1307304  994764  417992  178522%2C   91223  370224  376797  499509  151110  768095  322432  922943  943786%2C 1411824  535679  361596  634681  286016  467522  500940 1075759  980357%2C  551762 1439861 1012550 1072323  269188 1587473   25290  394744 1437961%2C  903215 1307291 1509762  512478 1181546  882275 1018292  952181 1488438%2C  799200 1279832 1374847  828887  210190  751091  258829  786389 1053678%2C  508688 1486966  598014 1342275  827170 1160837 1428720  674838  985315%2C  456857  298664 1298801  885372 1366327  259949  292449  961028 1180083%2C  292731  444653  874799  133278  420836  986335 1047904 1481895 1045111%2C 1085047 1442809  452878   57788 1561394 1506339  440018  197943 1240684%2C  480999 1048730  247191  278225  716223 1253081  331023  168486  796536%2C  129286%5D" isContainer="True" shape="(1440,)" />
<var name="_" type="tuple" qualifier="builtins" value="%281440%2C 160%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_12" type="int" qualifier="builtins" value="2" isIPythonHidden="True" />
<var name="_13" type="tuple" qualifier="builtins" value="%281440%2C 160%29" isContainer="True" shape="2" isIPythonHidden="True" />
<var name="_4" type="DataFrame" qualifier="pandas.core.frame" value="ID                                              Input  Target %5B1262547  1998859621  %40RSprung YES. let%27s do it. we%27ll chat soon abo...       1%5D %5B813847   1548911428  Back home%21  Bought a yummy sandwich on my way ...       1%5D %5B544315   2201072419  uh oh. my face feels a bit feverish. took my t...       0%5D %5B975173   1833483041  %40katfishh they promised the picture a live cha...       1%5D %5B1020704  1882429722  %40soully loved it  makes me want to watch the f...       1%5D" isContainer="True" shape="(5, 3)" isIPythonHidden="True" />
<var name="_5" type="int64" qualifier="numpy" value="4800" isContainer="True" shape="()" isIPythonHidden="True" />
<var name="_8" type="DataFrame" qualifier="pandas.core.frame" value="ID  Input %5Bdata_type Target            %5D %5Btest      0        80     80%5D %5B          1        80     80%5D %5Btrain     0       720    720%5D %5B          1       720    720%5D" isContainer="True" shape="(4, 2)" isIPythonHidden="True" />
<var name="__" type="int" qualifier="builtins" value="2" isIPythonHidden="True" />
<var name="___" type="DataFrame" qualifier="pandas.core.frame" value="ID  Input %5Bdata_type Target            %5D %5Btest      0        80     80%5D %5B          1        80     80%5D %5Btrain     0       720    720%5D %5B          1       720    720%5D" isContainer="True" shape="(4, 2)" isIPythonHidden="True" />
<var name="__builtin__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__builtins__" type="module" qualifier="builtins" value="%3Cmodule %27builtins%27 %28built-in%29&gt;" isContainer="True" isIPythonHidden="True" />
<var name="__doc__" type="str" qualifier="builtins" value="Automatically created module for IPython interactive environment" isIPythonHidden="True" />
<var name="__loader__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__name__" type="str" qualifier="builtins" value="__main__" isIPythonHidden="True" />
<var name="__package__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="__spec__" type="NoneType" qualifier="builtins" value="None" isIPythonHidden="True" />
<var name="_dh" type="list" qualifier="builtins" value="%5BPosixPath%28%27/Users/ishan/PycharmProjects/pythonProject1%27%29%5D" isContainer="True" shape="1" isIPythonHidden="True" />
<var name="_i" type="str" qualifier="builtins" value="from tqdm.notebook import tqdm%0Aimport wandb%0A%0Awandb.init%28project=%22sentiment-uda%22%2C entity=%22similarity-based-value-smoothing%22%29%0A%0Afor epoch in tqdm%28range%281%2Cepochs%2B1%29%29%3A%0A    model.train%28%29%0A%0A    loss_train_total=0%0A%0A    progress_bar = tqdm%28dataloader_train%2Cdesc = %22Epoch%3A %7B%3A1d%7D%22.format%28epoch%29%2Cleave = False%2Cdisable = False%29%0A%0A%0A    for batch in progress_bar%3A%0A        model.zero_grad%28%29%0A%0A        batch = tuple%28b.to%28device%29 for b in batch%29%0A%0A        inputs = %7B%0A            %22input_ids%22%3Abatch%5B0%5D%2C%0A            %22attention_mask%22%3Abatch%5B1%5D%2C%0A            %22labels%22%3Abatch%5B2%5D%0A%0A        %7D%0A        outputs = model%28%2A%2Ainputs%29%0A%0A        loss = outputs%5B0%5D%0A%23         logits = outputs%5B1%5D%0A        loss_train_total %2B=loss.item%28%29%0A        loss.backward%28%29%0A%0A        torch.nn.utils.clip_grad_norm%28model.parameters%28%29%2C1.0%29%0A%0A        optimizer.step%28%29%0A        scheduler.step%28%29%0A%0A%0A        progress_bar.set_postfix%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A        wandb.log%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A%23 ..." isIPythonHidden="True" />
<var name="_i1" type="str" qualifier="builtins" value="import numpy as np%0Aimport pandas as pd%0Afrom utils import %2A%0Aimport torch%0Afrom transformers import BertTokenizer%0Afrom torch.utils.data import TensorDataset" isIPythonHidden="True" />
<var name="_i10" type="str" qualifier="builtins" value="encoder_train = tokenizer.batch_encode_plus%28data_df%5Bdata_df%5B%22data_type%22%5D==%27train%27%5D.Input.values%2C%0A                                           add_special_tokens = True%2C%0A                                            return_attention_masks = True%2C%0A                                           pad_to_max_length = True%2C%0A                                           max_length = 256%2C%0A                                           return_tensors = %27pt%27%29%0A%0A%0A%0Aencoder_test = tokenizer.batch_encode_plus%28data_df%5Bdata_df%5B%22data_type%22%5D==%27test%27%5D.Input.values%2C%0A                                           add_special_tokens = True%2C%0A                                            return_attention_masks = True%2C%0A                                           pad_to_max_length = True%2C%0A                                           max_length = 256%2C%0A                                           return_tensors = %27pt%27%29%0A%0Ainput_ids_train = encoder_train%5B%27input_ids%27%5D%0Aattention_masks_train = encoder_train%5B%22attention_mask%22%5D%0Alabels_train = torch...." isIPythonHidden="True" />
<var name="_i11" type="str" qualifier="builtins" value="data_train = TensorDataset%28input_ids_train%2Cattention_masks_train%2Clabels_train%29%0Adata_test = TensorDataset%28input_ids_test%2Cattention_masks_test%2Clabels_test%29" isIPythonHidden="True" />
<var name="_i12" type="str" qualifier="builtins" value="len%28data_df.Target.unique%28%29%29" isIPythonHidden="True" />
<var name="_i13" type="str" qualifier="builtins" value="len%28data_train%29%2Clen%28data_test%29%23%25%25" isIPythonHidden="True" />
<var name="_i14" type="str" qualifier="builtins" value="from transformers import BertForSequenceClassification%0Amodel = BertForSequenceClassification.from_pretrained%28%27bert-base-uncased%27%2C%0A                                     num_labels = len%28data_df.Target.unique%28%29%29%2C%0A                                     output_attentions = False%2C%0A                                     output_hidden_states =  False%29" isIPythonHidden="True" />
<var name="_i15" type="str" qualifier="builtins" value="from torch.utils.data import RandomSampler%2CSequentialSampler%2CDataLoader%0A%0Adataloader_train = DataLoader%28%0A    data_train%2C%0A    sampler= RandomSampler%28data_train%29%2C%0A    batch_size = 16%0A%0A%29%0A%0A%0Adataloader_test = DataLoader%28%0A    data_test%2C%0A    sampler= RandomSampler%28data_test%29%2C%0A    batch_size = 32%0A%0A%29" isIPythonHidden="True" />
<var name="_i16" type="str" qualifier="builtins" value="from transformers import AdamW%2Cget_linear_schedule_with_warmup%0Aoptimizer = AdamW%28model.parameters%28%29%2Clr = 1e-5%2Ceps = 1e-8%29%0A%0Aepochs  = 10%0Ascheduler = get_linear_schedule_with_warmup%28%0A            optimizer%2C%0A    num_warmup_steps = 0%2C%0A   num_training_steps = len%28dataloader_train%29%2Aepochs%0A%29" isIPythonHidden="True" />
<var name="_i17" type="str" qualifier="builtins" value="from sklearn.metrics import f1_score%0A%0Adef f1_score_func%28preds%2Clabels%29%3A%0A    preds_flat = np.argmax%28preds%2Caxis=1%29.flatten%28%29%0A    labels_flat = labels.flatten%28%29%0A    return f1_score%28labels_flat%2Cpreds_flat%2Caverage = %27weighted%27%29" isIPythonHidden="True" />
<var name="_i18" type="str" qualifier="builtins" value="def accuracy_per_class%28preds%2Clabels%29%3A%0A    label_dict_reverse = %7Bv%3Ak for k%2Cv in dict_label.items%28%29%7D%0A%0A    preds_flat = np.argmax%28preds%2Caxis=1%29.flatten%28%29%0A    labels_flat = labels.flatten%28%29%0A%0A    for label in np.unique%28labels_flat%29%3A%0A        y_preds = preds_flat%5Blabels_flat==label%5D%0A        y_true = labels_flat%5Blabels_flat==label%5D%0A        print%28f%22Class%3A%7Blabel_dict_reverse%7D%22%29%0A        print%28f%22Accuracy%3A%7Blen%28y_preds%5By_preds==label%5D%29%7D/%7Blen%28y_true%29%7D%5Cn%22%29" isIPythonHidden="True" />
<var name="_i19" type="str" qualifier="builtins" value="import random%0A%0Aseed_val = 17%0Arandom.seed%28seed_val%29%0Anp.random.seed%28seed_val%29%0Atorch.manual_seed%28seed_val%29%0Atorch.cuda.manual_seed_all%28seed_val%29" isIPythonHidden="True" />
<var name="_i2" type="str" qualifier="builtins" value="import numpy as np%0Aimport pandas as pd%0Afrom utils import %2A%0Aimport torch%0Afrom transformers import BertTokenizer%0Afrom torch.utils.data import TensorDataset" isIPythonHidden="True" />
<var name="_i20" type="str" qualifier="builtins" value="device = torch.device%28%27cuda%27 if torch.cuda.is_available%28%29 else %27cpu%27%29%0Amodel.to%28device%29%0A%0Aprint%28f%22Loading%3A%7Bdevice%7D%22%29" isIPythonHidden="True" />
<var name="_i21" type="str" qualifier="builtins" value="def evaluate%28dataloader_val%29%3A%0A    model.eval%28%29%0A%0A    loss_val_total = 0%0A    predictions%2Ctrue_vals = %5B%5D%2C%5B%5D%0A%0A    for batch in tqdm%28dataloader_val%29%3A%0A        batch = tuple%28b.to%28device%29 for b in batch%29%0A%0A        inputs = %7B%27input_ids%27%3A  batch%5B0%5D%2C%0A                  %27attention_mask%27%3Abatch%5B1%5D%2C%0A                  %27labels%27%3A batch%5B2%5D%0A                 %7D%0A        with torch.no_grad%28%29%3A%0A            outputs = model%28%2A%2Ainputs%29%0A%0A        loss = outputs%5B0%5D%0A        logits = outputs%5B1%5D%0A        loss_val_total %2B=loss.item%28%29%0A%0A        logits = logits.detach%28%29.cpu%28%29.numpy%28%29%0A        label_ids = inputs%5B%27labels%27%5D.cpu%28%29.numpy%28%29%0A        predictions.append%28logits%29%0A        true_vals.append%28label_ids%29%0A%0A%0A    loss_val_avg = loss_val_total/len%28dataloader_val%29%0A%0A    predictions = np.concatenate%28predictions%2Caxis=0%29%0A    true_vals = np.concatenate%28true_vals%2Caxis=0%29%0A    return loss_val_avg%2Cpredictions%2Ctrue_vals" isIPythonHidden="True" />
<var name="_i22" type="str" qualifier="builtins" value="from tqdm.notebook import tqdm%0A%0Afor epoch in tqdm%28range%281%2Cepochs%2B1%29%29%3A%0A    model.train%28%29%0A%0A    loss_train_total=0%0A%0A    progress_bar = tqdm%28dataloader_train%2Cdesc = %22Epoch%3A %7B%3A1d%7D%22.format%28epoch%29%2Cleave = False%2Cdisable = False%29%0A%0A%0A    for batch in progress_bar%3A%0A        model.zero_grad%28%29%0A%0A        batch = tuple%28b.to%28device%29 for b in batch%29%0A%0A        inputs = %7B%0A            %22input_ids%22%3Abatch%5B0%5D%2C%0A            %22attention_mask%22%3Abatch%5B1%5D%2C%0A            %22labels%22%3Abatch%5B2%5D%0A%0A        %7D%0A        outputs = model%28%2A%2Ainputs%29%0A%0A        loss = outputs%5B0%5D%0A%23         logits = outputs%5B1%5D%0A        loss_train_total %2B=loss.item%28%29%0A        loss.backward%28%29%0A%0A        torch.nn.utils.clip_grad_norm%28model.parameters%28%29%2C1.0%29%0A%0A        optimizer.step%28%29%0A        scheduler.step%28%29%0A%0A%0A        progress_bar.set_postfix%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A%23     torch.save%28model.state_dict%28%29%2Cf%27/kaggle/output/BERT_ft_epoch%7Bepoch%7D.model%27%29To save the model after each epoch%0A%0A    tqdm.write%28%27%5CnEpoch %7Bepoch%7D%27%29%0A%0A    loss_train_avg =..." isIPythonHidden="True" />
<var name="_i23" type="str" qualifier="builtins" value="from tqdm.notebook import tqdm%0Aimport wandb%0A%0Awandb.init%28project=%22sentiment-uda%22%2C entity=%22similarity-based-value-smoothing%22%29%0A%0Afor epoch in tqdm%28range%281%2Cepochs%2B1%29%29%3A%0A    model.train%28%29%0A%0A    loss_train_total=0%0A%0A    progress_bar = tqdm%28dataloader_train%2Cdesc = %22Epoch%3A %7B%3A1d%7D%22.format%28epoch%29%2Cleave = False%2Cdisable = False%29%0A%0A%0A    for batch in progress_bar%3A%0A        model.zero_grad%28%29%0A%0A        batch = tuple%28b.to%28device%29 for b in batch%29%0A%0A        inputs = %7B%0A            %22input_ids%22%3Abatch%5B0%5D%2C%0A            %22attention_mask%22%3Abatch%5B1%5D%2C%0A            %22labels%22%3Abatch%5B2%5D%0A%0A        %7D%0A        outputs = model%28%2A%2Ainputs%29%0A%0A        loss = outputs%5B0%5D%0A%23         logits = outputs%5B1%5D%0A        loss_train_total %2B=loss.item%28%29%0A        loss.backward%28%29%0A%0A        torch.nn.utils.clip_grad_norm%28model.parameters%28%29%2C1.0%29%0A%0A        optimizer.step%28%29%0A        scheduler.step%28%29%0A%0A%0A        progress_bar.set_postfix%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A        wandb.log%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A%23 ..." isIPythonHidden="True" />
<var name="_i24" type="str" qualifier="builtins" value="from tqdm.notebook import tqdm%0Aimport wandb%0A%0Awandb.init%28project=%22sentiment-uda%22%2C entity=%22similarity-based-value-smoothing%22%29%0A%0Afor epoch in tqdm%28range%281%2Cepochs%2B1%29%29%3A%0A    model.train%28%29%0A%0A    loss_train_total=0%0A%0A    progress_bar = tqdm%28dataloader_train%2Cdesc = %22Epoch%3A %7B%3A1d%7D%22.format%28epoch%29%2Cleave = False%2Cdisable = False%29%0A%0A%0A    for batch in progress_bar%3A%0A        model.zero_grad%28%29%0A%0A        batch = tuple%28b.to%28device%29 for b in batch%29%0A%0A        inputs = %7B%0A            %22input_ids%22%3Abatch%5B0%5D%2C%0A            %22attention_mask%22%3Abatch%5B1%5D%2C%0A            %22labels%22%3Abatch%5B2%5D%0A%0A        %7D%0A        outputs = model%28%2A%2Ainputs%29%0A%0A        loss = outputs%5B0%5D%0A%23         logits = outputs%5B1%5D%0A        loss_train_total %2B=loss.item%28%29%0A        loss.backward%28%29%0A%0A        torch.nn.utils.clip_grad_norm%28model.parameters%28%29%2C1.0%29%0A%0A        optimizer.step%28%29%0A        scheduler.step%28%29%0A%0A%0A        progress_bar.set_postfix%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A        wandb.log%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A%23 ..." isIPythonHidden="True" />
<var name="_i3" type="str" qualifier="builtins" value="data_df = get_input_df%28%29%0Adata_df = data_df.sample%28frac=0.001%29" isIPythonHidden="True" />
<var name="_i4" type="str" qualifier="builtins" value="data_df.head%28%29" isIPythonHidden="True" />
<var name="_i5" type="str" qualifier="builtins" value="data_df.size" isIPythonHidden="True" />
<var name="_i6" type="str" qualifier="builtins" value="from sklearn.model_selection import train_test_split%0AX_train%2CX_test%2Cy_train%2Cy_test = train_test_split%28data_df.index.values%2C%0A                                                data_df.Target.values%2C%0A                                                test_size = 0.1%2C%0A                                                random_state=5%2C%0A                                                stratify = data_df.Target.values%29" isIPythonHidden="True" />
<var name="_i7" type="str" qualifier="builtins" value="data_df.loc%5BX_train%2C%27data_type%27%5D = %27train%27%0Adata_df.loc%5BX_test%2C%27data_type%27%5D = %27test%27" isIPythonHidden="True" />
<var name="_i8" type="str" qualifier="builtins" value="data_df.groupby%28%5B%27data_type%27%2C %27Target%27%5D%29.count%28%29" isIPythonHidden="True" />
<var name="_i9" type="str" qualifier="builtins" value="tokenizer = BertTokenizer.from_pretrained%28%27bert-base-uncased%27%2C%0A                                         do_lower_case = True%29" isIPythonHidden="True" />
<var name="_ih" type="list" qualifier="builtins" value="%5B%27%27%2C %27import numpy as np%5Cnimport pandas as pd%5Cnfrom utils import %2A%5Cnimport torch%5Cnfrom transformers import BertTokenizer%5Cnfrom torch.utils.data import TensorDataset%27%2C %27import numpy as np%5Cnimport pandas as pd%5Cnfrom utils import %2A%5Cnimport torch%5Cnfrom transformers import BertTokenizer%5Cnfrom torch.utils.data import TensorDataset%27%2C %27data_df = get_input_df%28%29%5Cndata_df = data_df.sample%28frac=0.001%29%27%2C %27data_df.head%28%29%27%2C %27data_df.size%27%2C %27from sklearn.model_selection import train_test_split%5CnX_train%2CX_test%2Cy_train%2Cy_test = train_test_split%28data_df.index.values%2C%5Cn                                                data_df.Target.values%2C%5Cn                                                test_size = 0.1%2C%5Cn                                                random_state=5%2C%5Cn                                                stratify = data_df.Target.values%29%27%2C %22data_df.loc%5BX_train%2C%27data_type%27%5D = %27train%27%5Cndata_df.loc%5BX_test%2C%27data_type%27%5D = %27test%27%22%2C %22data_df.groupby%28%5B%27data_type%27%2C %27Target%27%5D%29.count%28%29%22%2C %22tokenizer = Bert..." isContainer="True" shape="25" isIPythonHidden="True" />
<var name="_ii" type="str" qualifier="builtins" value="from tqdm.notebook import tqdm%0A%0Afor epoch in tqdm%28range%281%2Cepochs%2B1%29%29%3A%0A    model.train%28%29%0A%0A    loss_train_total=0%0A%0A    progress_bar = tqdm%28dataloader_train%2Cdesc = %22Epoch%3A %7B%3A1d%7D%22.format%28epoch%29%2Cleave = False%2Cdisable = False%29%0A%0A%0A    for batch in progress_bar%3A%0A        model.zero_grad%28%29%0A%0A        batch = tuple%28b.to%28device%29 for b in batch%29%0A%0A        inputs = %7B%0A            %22input_ids%22%3Abatch%5B0%5D%2C%0A            %22attention_mask%22%3Abatch%5B1%5D%2C%0A            %22labels%22%3Abatch%5B2%5D%0A%0A        %7D%0A        outputs = model%28%2A%2Ainputs%29%0A%0A        loss = outputs%5B0%5D%0A%23         logits = outputs%5B1%5D%0A        loss_train_total %2B=loss.item%28%29%0A        loss.backward%28%29%0A%0A        torch.nn.utils.clip_grad_norm%28model.parameters%28%29%2C1.0%29%0A%0A        optimizer.step%28%29%0A        scheduler.step%28%29%0A%0A%0A        progress_bar.set_postfix%28%7B%27training_loss%27%3A%27%7B%3A.3f%7D%27.format%28loss.item%28%29/len%28batch%29%29%7D%29%0A%23     torch.save%28model.state_dict%28%29%2Cf%27/kaggle/output/BERT_ft_epoch%7Bepoch%7D.model%27%29To save the model after each epoch%0A%0A    tqdm.write%28%27%5CnEpoch %7Bepoch%7D%27%29%0A%0A    loss_train_avg =..." isIPythonHidden="True" />
<var name="_iii" type="str" qualifier="builtins" value="def evaluate%28dataloader_val%29%3A%0A    model.eval%28%29%0A%0A    loss_val_total = 0%0A    predictions%2Ctrue_vals = %5B%5D%2C%5B%5D%0A%0A    for batch in tqdm%28dataloader_val%29%3A%0A        batch = tuple%28b.to%28device%29 for b in batch%29%0A%0A        inputs = %7B%27input_ids%27%3A  batch%5B0%5D%2C%0A                  %27attention_mask%27%3Abatch%5B1%5D%2C%0A                  %27labels%27%3A batch%5B2%5D%0A                 %7D%0A        with torch.no_grad%28%29%3A%0A            outputs = model%28%2A%2Ainputs%29%0A%0A        loss = outputs%5B0%5D%0A        logits = outputs%5B1%5D%0A        loss_val_total %2B=loss.item%28%29%0A%0A        logits = logits.detach%28%29.cpu%28%29.numpy%28%29%0A        label_ids = inputs%5B%27labels%27%5D.cpu%28%29.numpy%28%29%0A        predictions.append%28logits%29%0A        true_vals.append%28label_ids%29%0A%0A%0A    loss_val_avg = loss_val_total/len%28dataloader_val%29%0A%0A    predictions = np.concatenate%28predictions%2Caxis=0%29%0A    true_vals = np.concatenate%28true_vals%2Caxis=0%29%0A    return loss_val_avg%2Cpredictions%2Ctrue_vals" isIPythonHidden="True" />
<var name="_oh" type="dict" qualifier="builtins" value="%7B4%3A                  ID                                              Input  Target%0A1262547  1998859621  %40RSprung YES. let%27s do it. we%27ll chat soon abo...       1%0A813847   1548911428  Back home%21  Bought a yummy sandwich on my way ...       1%0A544315   2201072419  uh oh. my face feels a bit feverish. took my t...       0%0A975173   1833483041  %40katfishh they promised the picture a live cha...       1%0A1020704  1882429722  %40soully loved it  makes me want to watch the f...       1%2C 5%3A 4800%2C 8%3A                    ID  Input%0Adata_type Target            %0Atest      0        80     80%0A          1        80     80%0Atrain     0       720    720%0A          1       720    720%2C 12%3A 2%2C 13%3A %281440%2C 160%29%7D" isContainer="True" shape="5" isIPythonHidden="True" />
<var name="_pydevd_bundle" type="module" qualifier="builtins" value="%3Cmodule %27_pydevd_bundle%27 from %27/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/_pydevd_bundle/__init__.py%27&gt;" isContainer="True" />
<var name="accuracy_per_class" type="function" qualifier="builtins" value="%3Cfunction accuracy_per_class at 0x138d165e0&gt;" isContainer="True" />
<var name="attention_masks_test" type="Tensor" qualifier="torch" value="tensor%28%5B%5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%5D%29" isContainer="True" shape="(160, 256)" />
<var name="attention_masks_train" type="Tensor" qualifier="torch" value="tensor%28%5B%5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%5D%29" isContainer="True" shape="(1440, 256)" />
<var name="batch" type="tuple" qualifier="builtins" value="%28tensor%28%5B%5B  101%2C 11721%2C  2232%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C 19193%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C  2852%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        ...%2C%0A        %5B  101%2C  2065%2C  1045%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  8248%2C  2543%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2821%2C  4365%2C  ...%2C     0%2C     0%2C     0%5D%5D%29%2C tensor%28%5B%5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%5D%29%2C tensor%28%5B0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 1%2C 1%2C 1%2C 1%2C 1%2C 0%2C 1%2C 0%2C 0%2C 0%5D%29%29" isContainer="True" shape="3" />
<var name="data_df" type="DataFrame" qualifier="pandas.core.frame" value="ID                                              Input  %5C %5B1262547  1998859621  %40RSprung YES. let%27s do it. we%27ll chat soon abo...   %5D %5B813847   1548911428  Back home%21  Bought a yummy sandwich on my way ...   %5D %5B544315   2201072419  uh oh. my face feels a bit feverish. took my t...   %5D %5B975173   1833483041  %40katfishh they promised the picture a live cha...   %5D %5B1020704  1882429722  %40soully loved it  makes me want to watch the f...   %5D %5B...             ...                                                ...   %5D %5B63319    1687491016              %40SueRK I%27m jealous  mines been pants.   %5D %5B935262   1792695048  %40Linc4Justice  don%27t dispair%2C you got heaps of...   %5D %5B1408227  2055664170  %40SarahTasker seriously%3F%21 haha  next time i see...   %5D %5B1160837  1979459794  %40newcurator I%27m more of a Natural History muse...   %5D %5B1246726  1995384339  churccchhhhh%2C then Terra Nova Estrogen%2C then c...   %5D %5B%5D %5B         Target data_type  %5D %5B1262547       1     train  %5D %5B813847        1     train  %5D %5B544315        0     train  ...%5D" isContainer="True" shape="(1600, 4)" />
<var name="data_test" type="TensorDataset" qualifier="torch.utils.data.dataset" value="%3Ctorch.utils.data.dataset.TensorDataset object at 0x13e509e50&gt;" isContainer="True" shape="160" />
<var name="data_train" type="TensorDataset" qualifier="torch.utils.data.dataset" value="%3Ctorch.utils.data.dataset.TensorDataset object at 0x13e509f10&gt;" isContainer="True" shape="1440" />
<var name="dataloader_test" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1416e5250&gt;" isContainer="True" shape="5" />
<var name="dataloader_train" type="DataLoader" qualifier="torch.utils.data.dataloader" value="%3Ctorch.utils.data.dataloader.DataLoader object at 0x1416e5310&gt;" isContainer="True" shape="90" />
<var name="device" type="device" qualifier="torch" value="cpu" isContainer="True" />
<var name="encoder_test" type="BatchEncoding" qualifier="transformers.tokenization_utils_base" value="%7B%27input_ids%27%3A tensor%28%5B%5B  101%2C  1030%2C 11382%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2026%2C  2132%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2085%2C  1045%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        ...%2C%0A        %5B  101%2C  1030%2C 27485%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C  6175%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C 10882%2C  ...%2C     0%2C     0%2C     0%5D%5D%29%2C %27token_type_ids%27%3A tensor%28%5B%5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%5D%29%2C %27attention_mask%27%3A tensor%28%5B%5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%5D%29%7D" isContainer="True" shape="3" />
<var name="encoder_train" type="BatchEncoding" qualifier="transformers.tokenization_utils_base" value="%7B%27input_ids%27%3A tensor%28%5B%5B  101%2C  1030%2C 12667%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2067%2C  2188%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  7910%2C  2821%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        ...%2C%0A        %5B  101%2C  1030%2C  4532%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C  2047%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C 14684%2C 11890%2C  ...%2C     0%2C     0%2C     0%5D%5D%29%2C %27token_type_ids%27%3A tensor%28%5B%5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B0%2C 0%2C 0%2C  ...%2C 0%2C 0%2C 0%5D%5D%29%2C %27attention_mask%27%3A tensor%28%5B%5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%5D%29%7D" isContainer="True" shape="3" />
<var name="epoch" type="int" qualifier="builtins" value="1" />
<var name="epochs" type="int" qualifier="builtins" value="10" />
<var name="evaluate" type="function" qualifier="builtins" value="%3Cfunction evaluate at 0x1416c6940&gt;" isContainer="True" />
<var name="exit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x10ffdd040&gt;" isContainer="True" isIPythonHidden="True" />
<var name="f1_score" type="function" qualifier="builtins" value="%3Cfunction f1_score at 0x142a54a60&gt;" isContainer="True" />
<var name="f1_score_func" type="function" qualifier="builtins" value="%3Cfunction f1_score_func at 0x140e7b4c0&gt;" isContainer="True" />
<var name="flags" type="dict" qualifier="builtins" value="%7B%27input_path%27%3A %27sentiment_data.csv%27%7D" isContainer="True" shape="1" />
<var name="get_input_df" type="function" qualifier="builtins" value="%3Cfunction get_input_df at 0x12b8efee0&gt;" isContainer="True" />
<var name="get_ipython" type="method" qualifier="builtins" value="%3Cbound method InteractiveShell.get_ipython of %3Cipykernel.zmqshell.ZMQInteractiveShell object at 0x10ffcc7c0&gt;&gt;" isContainer="True" isIPythonHidden="True" />
<var name="get_linear_schedule_with_warmup" type="function" qualifier="builtins" value="%3Cfunction get_linear_schedule_with_warmup at 0x141700d30&gt;" isContainer="True" />
<var name="input_ids_test" type="Tensor" qualifier="torch" value="tensor%28%5B%5B  101%2C  1030%2C 11382%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2026%2C  2132%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2085%2C  1045%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        ...%2C%0A        %5B  101%2C  1030%2C 27485%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C  6175%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C 10882%2C  ...%2C     0%2C     0%2C     0%5D%5D%29" isContainer="True" shape="(160, 256)" />
<var name="input_ids_train" type="Tensor" qualifier="torch" value="tensor%28%5B%5B  101%2C  1030%2C 12667%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2067%2C  2188%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  7910%2C  2821%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        ...%2C%0A        %5B  101%2C  1030%2C  4532%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C  2047%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C 14684%2C 11890%2C  ...%2C     0%2C     0%2C     0%5D%5D%29" isContainer="True" shape="(1440, 256)" />
<var name="inputs" type="dict" qualifier="builtins" value="%7B%27input_ids%27%3A tensor%28%5B%5B  101%2C 11721%2C  2232%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C 19193%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  1030%2C  2852%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        ...%2C%0A        %5B  101%2C  2065%2C  1045%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  8248%2C  2543%2C  ...%2C     0%2C     0%2C     0%5D%2C%0A        %5B  101%2C  2821%2C  4365%2C  ...%2C     0%2C     0%2C     0%5D%5D%29%2C %27attention_mask%27%3A tensor%28%5B%5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        ...%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%2C%0A        %5B1%2C 1%2C 1%2C  ...%2C 0%2C 0%2C 0%5D%5D%29%2C %27labels%27%3A tensor%28%5B0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 1%2C 1%2C 1%2C 1%2C 1%2C 0%2C 1%2C 0%2C 0%2C 0%5D%29%7D" isContainer="True" shape="3" />
<var name="labels_test" type="Tensor" qualifier="torch" value="tensor%28%5B1%2C 0%2C 0%2C 1%2C 0%2C 1%2C 0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 1%2C 0%2C 1%2C 0%2C 1%2C 0%2C 0%2C 0%2C%0A        0%2C 0%2C 0%2C 0%2C 0%2C 0%2C 1%2C 1%2C 0%2C 1%2C 0%2C 0%2C 0%2C 1%2C 1%2C 0%2C 1%2C 1%2C 1%2C 1%2C 1%2C 0%2C 1%2C 0%2C%0A        0%2C 0%2C 0%2C 1%2C 0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 0%2C 0%2C 1%2C 0%2C 1%2C 1%2C 0%2C 1%2C 1%2C 1%2C 1%2C 0%2C 0%2C 1%2C%0A        1%2C 1%2C 0%2C 0%2C 1%2C 0%2C 0%2C 0%2C 0%2C 0%2C 1%2C 0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 0%2C 0%2C 1%2C 0%2C 0%2C 1%2C 0%2C%0A        1%2C 0%2C 0%2C 0%2C 1%2C 1%2C 1%2C 1%2C 0%2C 0%2C 1%2C 0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 0%2C 1%2C 1%2C 1%2C 1%2C 1%2C 1%2C%0A        1%2C 0%2C 1%2C 1%2C 0%2C 1%2C 0%2C 0%2C 1%2C 1%2C 1%2C 1%2C 0%2C 1%2C 1%2C 0%2C 0%2C 0%2C 1%2C 1%2C 0%2C 0%2C 0%2C 1%2C%0A        0%2C 0%2C 1%2C 0%2C 1%2C 1%2C 0%2C 0%2C 1%2C 1%2C 1%2C 0%2C 1%2C 1%2C 0%2C 1%5D%29" isContainer="True" shape="(160,)" />
<var name="labels_train" type="Tensor" qualifier="torch" value="tensor%28%5B1%2C 1%2C 0%2C  ...%2C 1%2C 1%2C 1%5D%29" isContainer="True" shape="(1440,)" />
<var name="loss" type="Tensor" qualifier="torch" value="tensor%280.7200%2C grad_fn=%3CNllLossBackward0&gt;%29" isContainer="True" shape="()" />
<var name="loss_train_total" type="float" qualifier="builtins" value="0.7199767827987671" />
<var name="model" type="BertForSequenceClassification" qualifier="transformers.models.bert.modeling_bert" value="BertForSequenceClassification%28%0A  %28bert%29%3A BertModel%28%0A    %28embeddings%29%3A BertEmbeddings%28%0A      %28word_embeddings%29%3A Embedding%2830522%2C 768%2C padding_idx=0%29%0A      %28position_embeddings%29%3A Embedding%28512%2C 768%29%0A      %28token_type_embeddings%29%3A Embedding%282%2C 768%29%0A      %28LayerNorm%29%3A LayerNorm%28%28768%2C%29%2C eps=1e-12%2C elementwise_affine=True%29%0A      %28dropout%29%3A Dropout%28p=0.1%2C inplace=False%29%0A    %29%0A    %28encoder%29%3A BertEncoder%28%0A      %28layer%29%3A ModuleList%28%0A        %280%29%3A BertLayer%28%0A          %28attention%29%3A BertAttention%28%0A            %28self%29%3A BertSelfAttention%28%0A              %28query%29%3A Linear%28in_features=768%2C out_features=768%2C bias=True%29%0A              %28key%29%3A Linear%28in_features=768%2C out_features=768%2C bias=True%29%0A              %28value%29%3A Linear%28in_features=768%2C out_features=768%2C bias=True%29%0A              %28dropout%29%3A Dropout%28p=0.1%2C inplace=False%29%0A            %29%0A            %28output%29%3A BertSelfOutput%28%0A              %28dense%29%3A Linear%28in_features=768%2C out_features=768%2C bias=True%29%0A              %28LayerNorm%29%3A LayerNorm%28%28768%2C%29%2C eps=1e-12%2C element..." isContainer="True" />
<var name="np" type="module" qualifier="builtins" value="%3Cmodule %27numpy%27 from %27/Users/ishan/PycharmProjects/pythonProject1/venv/lib/python3.8/site-packages/numpy/__init__.py%27&gt;" isContainer="True" />
<var name="optimizer" type="AdamW" qualifier="transformers.optimization" value="AdamW %28%0AParameter Group 0%0A    betas%3A %280.9%2C 0.999%29%0A    correct_bias%3A True%0A    eps%3A 1e-08%0A    initial_lr%3A 1e-05%0A    lr%3A 9.955555555555556e-06%0A    weight_decay%3A 0.0%0A%29" isContainer="True" />
<var name="outputs" type="SequenceClassifierOutput" qualifier="transformers.modeling_outputs" value="SequenceClassifierOutput%28loss=tensor%280.7200%2C grad_fn=%3CNllLossBackward0&gt;%29%2C logits=tensor%28%5B%5B-0.2908%2C -0.1032%5D%2C%0A        %5B-0.2434%2C  0.0700%5D%2C%0A        %5B-0.4824%2C -0.1084%5D%2C%0A        %5B-0.2623%2C -0.1241%5D%2C%0A        %5B-0.1243%2C -0.2088%5D%2C%0A        %5B-0.2213%2C -0.2444%5D%2C%0A        %5B-0.1440%2C -0.1253%5D%2C%0A        %5B 0.0241%2C  0.3809%5D%2C%0A        %5B-0.0756%2C -0.1896%5D%2C%0A        %5B-0.1581%2C  0.0063%5D%2C%0A        %5B-0.0404%2C  0.2339%5D%2C%0A        %5B-0.2363%2C -0.1708%5D%2C%0A        %5B-0.3993%2C -0.2106%5D%2C%0A        %5B-0.1263%2C  0.0387%5D%2C%0A        %5B-0.3593%2C -0.3208%5D%2C%0A        %5B-0.3012%2C -0.0784%5D%5D%2C grad_fn=%3CAddmmBackward0&gt;%29%2C hidden_states=None%2C attentions=None%29" isContainer="True" shape="2" />
<var name="pd" type="module" qualifier="builtins" value="%3Cmodule %27pandas%27 from %27/Users/ishan/PycharmProjects/pythonProject1/venv/lib/python3.8/site-packages/pandas/__init__.py%27&gt;" isContainer="True" />
<var name="progress_bar" type="tqdm_notebook" qualifier="tqdm.notebook" value="Epoch%3A 1%3A   1%25%7C          %7C 1/90 %5B01%3A23%3C1%3A37%3A09%2C 65.50s/it%2C training_loss=0.240%5D" isContainer="True" shape="90" />
<var name="pydev_jupyter_vars" type="module" qualifier="builtins" value="%3Cmodule %27pydev_jupyter_vars%27 from %27/Applications/PyCharm.app/Contents/plugins/python/helpers-pro/jupyter_debug/pydev_jupyter_vars.py%27&gt;" isContainer="True" />
<var name="quit" type="ZMQExitAutocall" qualifier="IPython.core.autocall" value="%3CIPython.core.autocall.ZMQExitAutocall object at 0x10ffdd040&gt;" isContainer="True" isIPythonHidden="True" />
<var name="random" type="module" qualifier="builtins" value="%3Cmodule %27random%27 from %27/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.8/lib/python3.8/random.py%27&gt;" isContainer="True" />
<var name="remove_imported_pydev_package" type="function" qualifier="builtins" value="%3Cfunction remove_imported_pydev_package at 0x12d1144c0&gt;" isContainer="True" />
<var name="scheduler" type="LambdaLR" qualifier="torch.optim.lr_scheduler" value="%3Ctorch.optim.lr_scheduler.LambdaLR object at 0x1416ff5e0&gt;" isContainer="True" />
<var name="seed_val" type="int" qualifier="builtins" value="17" />
<var name="sys" type="module" qualifier="builtins" value="%3Cmodule %27sys%27 %28built-in%29&gt;" isContainer="True" />
<var name="tokenizer" type="BertTokenizer" qualifier="transformers.models.bert.tokenization_bert" value="PreTrainedTokenizer%28name_or_path=%27bert-base-uncased%27%2C vocab_size=30522%2C model_max_len=512%2C is_fast=False%2C padding_side=%27right%27%2C truncation_side=%27right%27%2C special_tokens=%7B%27unk_token%27%3A %27%5BUNK%5D%27%2C %27sep_token%27%3A %27%5BSEP%5D%27%2C %27pad_token%27%3A %27%5BPAD%5D%27%2C %27cls_token%27%3A %27%5BCLS%5D%27%2C %27mask_token%27%3A %27%5BMASK%5D%27%7D%29" isContainer="True" shape="30522" />
<var name="torch" type="module" qualifier="builtins" value="%3Cmodule %27torch%27 from %27/Users/ishan/PycharmProjects/pythonProject1/venv/lib/python3.8/site-packages/torch/__init__.py%27&gt;" isContainer="True" />
<var name="tqdm" type="type" qualifier="builtins" value="%3Cclass %27tqdm.notebook.tqdm_notebook%27&gt;" isContainer="True" />
<var name="train_test_split" type="function" qualifier="builtins" value="%3Cfunction train_test_split at 0x142b89670&gt;" isContainer="True" />
<var name="wandb" type="module" qualifier="builtins" value="%3Cmodule %27wandb%27 from %27/Users/ishan/PycharmProjects/pythonProject1/venv/lib/python3.8/site-packages/wandb/__init__.py%27&gt;" isContainer="True" />
<var name="y_test" type="ndarray" qualifier="numpy" value="%5B0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 1 0 1 1 1 1 0 0 1 1 1 1 0 0 0 1 0 1 1 1 1%2C 0 1 1 1 0 1 1 0 0 0 0 0 1 1 0 0 0 1 0 1 1 0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1%2C 1 0 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 1 0 1 0 1 1 1 0 0%5D" isContainer="True" shape="(160,)" />
<var name="y_train" type="ndarray" qualifier="numpy" value="%5B0 0 1 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 1 1%2C 1 1 0 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 1 0 1 1 1 1 0 1 0 0 1 1 1 0 0 1 1 0 0%2C 1 0 0 1 1 1 1 1 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0 0 0 0%5D" isContainer="True" shape="(1440,)" />
</xml>